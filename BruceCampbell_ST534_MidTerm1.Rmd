---
title: "Bruce Campell NCSU ST 534 Exam 1"
date: "`r format(Sys.time(), '%d %B, %Y')`"
fontsize: 12pt
header-includes:
   - \usepackage{bbm}
output: pdf_document
---

---
```{r setup, include=FALSE,echo=FALSE}
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(dev = 'pdf')
knitr::opts_chunk$set(cache=TRUE)
knitr::opts_chunk$set(tidy=TRUE)
knitr::opts_chunk$set(prompt=FALSE)
knitr::opts_chunk$set(fig.height=5)
knitr::opts_chunk$set(fig.width=7)
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_chunk$set(message=FALSE)
knitr::opts_knit$set(root.dir = ".")
library(latex2exp)   
library(pander)
library(ggplot2)
library(ggplot2)
library(GGally)
```

# 1 Periodic Series With Noise

Generate a time series x of length 500 using the following R commands:

```{r}
t<-seq(1:500)
x<-cos(2* pi* .2 *t)+ (1-t/250)^3+rnorm(500)
x<-ts(x)
x.no.noise <- cos(2* pi* .2 *t)+ (1-t/250)^3
```

## (a) Plot x. Does the plot appear to be stationary?

```{r,echo=FALSE}
plot(x)
sm <-mean(x)
plot(x, main = "time series along with sample mean and underlying noisless series")
points(t,x.no.noise,col='blue',pch='*')
abline(h=sm, col='red')
legend("topright", title.col = "black",c("time series","no noise term","sample mean" ), text.col =c("black","blue","red"))
```

This time series does not appear stationary, The mean function appears to have drift and the variance does not appear constant. 

## (b) Plot the ACF of x. What feature(s) do(es) the acf-plot reveal ? 

```{r echo=FALSE}
acf(x)
```

We see negative autocorrelation at $lags 5n+2,5n+3 n=0,1,2..$. We see positive autocorrelation at $lags 5n+4,5n+5,5n+6 n=0,1,2,...$ with the strongest peak at $lag 5$. 


## (c) Plot x and overlay two kernel smoothed curves using the Gaussian ("normal") kernel with two different choices of the bandwidth b = 30; 50. 

```{r, echo=FALSE}
plot(x,main="Gaussian Kernel Smoothed")
lines(ksmooth(time(x),x, "normal", bandwidth = 30), col='red')
lines(ksmooth(time(x),x, "normal", bandwidth = 50), col='blue')
legend("topright", title.col = "black",c("time series","bandwidth=30","bandwidth=50" ), text.col =c("black","red","blue"))

#lines(ksmooth(time(x),x, "normal", bandwidth = 1/.2), col='cyan')
```

## (d) Which of the two smoothed curves (or the bandwidths) gives a better description of the "trend function" $g(t) = (1-\frac{t}{250})$ ? Justify your answer by relating the bandwidth choice with the period of the cyclical component.

The kernel with the larger bandwidth will capture the trend better. For ksmooth in R the kernels are scaled so that their quartiles are at  0.25*bandwidth.  The period of the periodic component of the series is 5 for so a kernel width of 50 will cover 5 periods in the interqualrtile range and 10 over the entire bandwidth.  This will smooth out the periodic fluctuations revealing the underlying trend better.  

#2 Modelling the varve series 

Consider the time series varve given in the package ASTSA.

## (a) Show that the varve series is heteroscedastic by computing the sample variances
over the first and the second half of the data. 

```{r}
rm(list = ls())
library(astsa)
data(varve, package="astsa")
plot(ts(varve))
n.tics <- length(varve)

left.half <- window(varve,start = 1,end = floor(length(varve) /2))
right.half <- window (varve,start=floor(length(varve) /2) +1, end = length(varve))

mean.left <- mean(left.half)
mean.right <- mean(right.half)
pander(data.frame(mean.left = mean.left, mean.right = mean.right), caption = "Sample means for right and left half of varve ts")

var.left <- var(left.half)
var.right <- var(right.half)

pander(data.frame(var.left = var.left, var.right = var.right), caption = "Sample variances for right and left half of varve ts")

var.test(left.half,right.half)
```

The F-test confirms what we see - that the variances of the left and right halves are significantly different.

## (b) Let x1 denote the first half of the varve series scaled by the sample standard
deviation of the first half, and similarly, let x2 denote the second half of the
varve series scaled by the sample standard deviation of the second half. Plot the
two subseries x1 and x2 in two panels using the plotting function mfrow=c(2,1).

```{r}
x1 <- left.half
x2 <- right.half
x1.scaled<- scale(left.half,center = FALSE,scale = sqrt(var.left))
x2.scaled<- scale(right.half,center = FALSE, scale = sqrt(var.right))

par(mfrow=c(2,1), mar=c(3,2,1,0)+.5, mgp=c(1.6,.6,0))
plot(ts(x1.scaled),ylab="x1")
plot(ts(x2.scaled),ylab="x2")

```

## (c) Now combine the two scaled series $x1$ and $x2$, and call it $xt$. Plot the ACF of the $xt$ series and comment on its (non-)stationarity properties! 

```{r}

union.x <- ts(union(x1.scaled,x2.scaled))
plot(union.x)
acf(union.x)
```

Since the acf is slowly decaying - we have evidence of non-stationarity. We can't say for sure though, but we should definitely investigate for trend.  If we were taking the follow up course - we'd be curious about unit root test - I believe these are for detecting long range correlations in time series.


## (d) Consider the differenced time series $xdiff$ obtained from $xt$. Show that an $MA(1)$ model is appropriate for $xdiff$. 

```{r}
diff.union.x <- ts(diff(union.x))
plot(diff.union.x, main="first difference")
acf(diff.union.x, 20, main="first difference")
```


We see in autocorrelation plot of the differenced data with a 95% confidence bands that the autocorrelation at lag 1 is significant.  Based on this a MA(1) model is suggested.  If we wanted to we could investigate the PACF to look for an AR component.  

There are two other significant ACF values at lag 9 and 10. They are of differing sign, and they are not that far above the $\alpha=0.05$ line so we claim there is no need to include them in our modelling process at this point. 

Now we run a Box-Ljung test to see if we can fit a MA model to the data. 

```{r}
union.x.ma1 <-arima(x =union.x, order = c(p = 0, d =
0, q = 1))
Box.test(residuals(union.x.ma1), lag = 6, type ="Ljung")
```
The Box-Ljung test shows that the lag autocorrelations among the residuals hence the $MA(1)$ model provides a good fit to the data. 

## (e) The model for $X_t = xdiff$ can be written as 
$$X_t = \mu + W_t + \theta_1 W_{t-1}$$ 
Where ${W_t} \sim N(-,\sigma_W)$.  Find an estimate of $\mu$.


Since $E[W_t] = 0 \;\;\forall t$ We have that $E[X_t]=\mu$ and it stands to reason that $\hat{\mu}=\bar{x_t}$ is a good estimate of $\mu$

```{r}
pander(data.frame(mean.xdiff=mean(diff.union.x)),capiton = "Estimate of constant in MA model")
```

It's interesting to look at the trend in the original series. We can fit a linear model $\mu_t = \beta_0 + \beta_1 t$ and see the trend in the original series.  I'd be curious to understand how differencing compares to subtracting the linear trend in terms of model fit - likewise for the scaling we did to remove the heteroskedasticity. We'd consider a log or square root transform as an alternative for the left right scaling.   

```{r}
fit = lm(union.x~time(union.x), na.action=NULL) 
summary(fit)
mean(union.x)
plot(union.x)
lines(fitted(fit), col='red')
```


## (f) Write down the final model for the varve-series based on this analysis.


We can use the autocovariance function

$$\gamma(h) = 
\begin{cases}
\sigma_w^2 (1+\theta^2) & h=0 \\
-\theta \;\sigma_w^2 & h=1 \\
0 & h>1
\end{cases}
$$
to match up with the sample autocovariance and get estimates for $\theta$ and $\sigma_w^2$

```{r}
autocov <- acf(diff.union.x, type = "covariance")

```
We'll extract the values for $\gamma(0)$ and $\gamma(1)$ to get 

$\sigma_w^2 (1+\theta^2)=$ 

```{r}
autocov[0]
```

and 

$-\theta \;\sigma_w^2=$  

```{r} 
autocov[1]
```

Solving these 

```{r}
a=autocov$acf[2]
b=autocov$acf[1]
c=autocov$acf[2]
first.root <- ((-b) + sqrt((b^2) - 4*a*c)) / (2*a)
second.root <- ((-b) - sqrt((b^2) - 4*a*c)) / (2*a)

pander(data.frame(first.root=first.root,second.root=second.root), caption="roots")

pander(data.frame(sima.hat=(-autocov$acf[2]/first.root)),caption="estimated variance of W_t from our data from first root")

pander(data.frame(sima.hat=(-autocov$acf[2]/second.root)),caption="estimated variance of W_t from our data from second root")
```

Now we have 2 choices of root above which give 2 equivalent models with different noise processes.  We choose $\hat{theta}=0.5107369$ so the time series will be invertible. Note this value of $\theta$ gives us $\sigma_w^2 = \frac{-\gamma(1)}{\theta}=.888$


Now that we have $\hat{\theta}=0.5107369$ and $\hat{\delta}= -0.003011$ we can back out our scaling and diff to get an expression for the original varve time series model.

A log transform for the scaling would be more elegant to look at but here we go

Let 

$$f(y_t)= 
\begin{cases}
y_t \frac{1}{\hat{\sigma_L}} &  t< \frac{length}{2}\\
\\
y_t \frac{1}{\hat{\sigma_L}} &  t< \frac{length}{2}\\
\end{cases}
$$
Where we got the scaling from the left and right sample variances.  We denote the inverse scaling by $f^{-1}$  


$$f^{-1}(z_y)= 
\begin{cases}
z_t \hat{\sigma_L} & t< \frac{length}{2}\\
\\
z_t \hat{\sigma_L} &  t< \frac{length}{2}\\
\end{cases}
$$

Or model for the diff is 

$$xdiff_t = f(y_t)- f(y_{t-1}) = \mu + W_t + \theta_1 W_{t-1}$$ 

The final model is then

$$y_t = y_{t-1} + f^{-1} ( \mu + W_t + \theta_1 W_{t-1}) $$

# 3 ARIMA Question

Suppose that $X_t$ is an $ARMA(p,q)$ process:
$$X_t = .5 X_{t-1} + W_t -0.7 W_{t-1} + 0.1 W_{t-2}$$
where $W_t$ are IID $N(0,1)$.

## (a) Find $p$ and $q$. (Be sure to check for model redundancy.)

From our structural representation via the backshift operator $\phi(B)X_t = \theta(B) W_t$ we get the polynomials

$\phi(z)=(1-.5z)$ and $\theta(z)= (1 -0.7 z + 0.1 z^2)$ checking for common roots

We see that $\theta(z)= (1-0.5 z)(1-0.2 z)$ so there is a common factor of $(1-0.5z)$

This means our model is a ARMA(0,2)=MA(1) rather than a ARMA(2,2).  

## (b) Show that $X_t$ is invertible.

Since the roots of $\theta(z)$ are outside the unit circle, the ARIMA process is invertible. 

## (c) Find the ACF of ${X_t}$ explicitly.

$X_t = W_t -0.2 W_{t-1}$ 

We have $E(X_t) =0$ and putting or model into $cov(x_{t+h},x_t)$ and collecting terms we have that

$$\gamma(h)= 
\begin{cases}
\sigma_w^2 \; (1+\theta^2)&  h=0\\
\sigma_w^2 \; \theta &  h=1\\
0 & h>1
\end{cases}
$$
$$\gamma(h)= 
\begin{cases}
\sigma_w^2 \; 1.04 &  h=0\\
\sigma_w^2 \; 0.2 &  h=1\\
0 & h>1
\end{cases}
$$
From which we get

$$\rho(h)= 
\begin{cases}
1 &  h=0\\
 0.1923077 &  h=1\\
0 & h>1
\end{cases}
$$


Originally I worked through the problem as if there were no shared term.  The recipe below is what I put together given that the roots for $\phi(z)$ and $\theta(z)$ lie outside the unit circle. 

Since the root of $\phi(z)$ lies outside the unit circle, the process is causal and we can write the time series as a one sided linear process $x_t = \psi(B) w_t$ from which we have $E[x_t]=0$.  So $\gamma(h) = cov(x_{t+h},x_t) = \sigma_w \sum\limits_{j=0}^{\infty} \psi_{j+h}\psi_j$ 

For a causal ARIMA model we have that the coefficients must satisfy 

$$\phi(z)\psi(x)=\theta(z)$$ and we can preform a coefficient matching to extract the values of $\psi_j$ to use in $\gamma(h)$

$$(\psi_0 + \psi_1 z + \psi_2 z^2 ) (\phi_0 +\phi_1 z ) = (\theta_0+\theta_1 z +\theta_2 z^2)$$
Putting this in the expression for $\gamma(h)$ we have that 

$$\gamma(h)= 
\begin{cases}
\sigma_w^2 \; (\psi_0^2 + \psi_1^2 + \psi_2^2) &  h=0\\
\sigma_w^2 \; (\psi_0 \psi_1 + \psi_1 \psi_2) &  h=1\\
\sigma_w^2 \; (\psi_0 \psi_2) &  h=2\\
0 & h>2
\end{cases}
$$
and 

$$\rho(h)= 
\begin{cases}
1 &  h=0\\
\\
\frac{(\psi_0 \psi_1 + \psi_1 \psi_2)}{(\psi_0^2 + \psi_1^2 + \psi_2^2)} &  h=1\\
\\
\frac{(\psi_0 \psi_2)}{(\psi_0^2 + \psi_1^2 + \psi_2^2)} &  h=2\\
0 & h>2
\end{cases}
$$





















